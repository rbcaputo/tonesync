using AudioToolbox;
using AVFoundation;
using Foundation;
using Microsoft.Extensions.Logging;

namespace ToneSync.App.Services
{
  /// <summary>
  /// iOS-specific audio implementation using AVAudioEngine.
  /// Optimized for Core Audio real-time rendering.
  /// </summary>
  public sealed partial class AudioService
  {
    private AVAudioEngine? _avAudioEngine;
    private AVAudioSourceNode? _sourceNode;
    private AVAudioFormat? _audioFormat;

    // Pre-allocated render buffer (no allocations in callback)
    private readonly float[] _iosRenderBuffer = new float[ToneSync.Core.AudioSettings.MaxBufferSize];

    partial void InitializePlatformAudio()
    {
      _logger.LogInformation("Initializing iOS AVAudioEngine");

      try
      {
        // Configure audio session for playback
        AVAudioSession audioSession = AVAudioSession.SharedInstance();

        // SetCategory requires NSString, not AVAudioSessionCategory enum
        bool categoryResult = audioSession.SetCategory(
          AVAudioSession.CategoryPlayback,
          out NSError sessionError
        );

        if (!categoryResult || sessionError is not null)
          throw new InvalidOperationException(
            $"Failed to set audio category: {sessionError?.LocalizedDescription ?? "Unknown error"}"
          );

        bool activeResult = audioSession.SetActive(true, out sessionError);

        if (!activeResult || sessionError is not null)
          throw new InvalidOperationException(
            $"Failed to activate audio session: {sessionError?.LocalizedDescription ?? "Unknown error"}"
          );

        _logger.LogDebug("Audio session configured: Category=Playback");

        // Create audio engine
        _avAudioEngine = new();

        // Create audio format (mono, 44.1kHz, float32)
        _audioFormat = new(
          sampleRate: ToneSync.Core.AudioSettings.SampleRate,
          channels: 1
        );

        if (_audioFormat is null)
          throw new InvalidOperationException("Failed to create audio format");

        _logger.LogDebug(
          "Audio format: SampleRate={SampleRate}, Channels={Channels}",
          _audioFormat.SampleRate,
          _audioFormat.ChannelCount
        );

        // Create source node with render callback
        _sourceNode = new((
          ref isSilence,
          ref timestamp,
          frameCount,
          audioBuffers
        ) => RenderBlock(ref isSilence, ref timestamp, frameCount, audioBuffers));

        if (_sourceNode is null)
          throw new InvalidOperationException("Failed to create source node");

        // Attach node to engine
        _avAudioEngine.AttachNode(_sourceNode);

        // Connect source node to main mixer
        _avAudioEngine.Connect(
          _sourceNode,
          _avAudioEngine.MainMixerNode,
          _audioFormat
        );

        // Prepare engine for playback
        _avAudioEngine.Prepare();

        _logger.LogInformation("iOS AVAudioEngine initialized successfully");
      }
      catch (Exception ex)
      {
        _logger.LogError(ex, "Failed to initialize iOS audio");
        throw new InvalidOperationException("iOS audio initialization failed", ex);
      }
    }

    partial void StartPlatformAudio()
    {
      if (_avAudioEngine is null || _avAudioEngine.Running)
        return;

      _logger.LogInformation("Starting iOS audio engine");

      try
      {
        _avAudioEngine.StartAndReturnError(out NSError? error);

        if (error is not null)
          throw new InvalidOperationException(
            $"Failed to start audio engine: {error.LocalizedDescription}"
          );

        _logger.LogInformation("iOS audio engine started");
      }
      catch (Exception ex)
      {
        _logger.LogError(ex, "Failed to start iOS audio");
        throw new InvalidOperationException("Failed to start iOS audio", ex);
      }
    }

    partial void StopPlatformAudio()
    {
      if (_avAudioEngine is null || !_avAudioEngine.Running)
        return;

      _logger.LogInformation("Stopping iOS audio engine");

      try
      {
        _avAudioEngine.Stop();
        _logger.LogInformation("iOS audio engine stopped");
      }
      catch (Exception ex)
      {
        _logger.LogError(ex, "Error stopping iOS audio");
      }
    }

    partial void DisposePlatformAudio()
    {
      try
      {
        if (_avAudioEngine?.Running == true)
          _avAudioEngine.Stop();

        _sourceNode?.Dispose();
        _avAudioEngine?.Dispose();

        _sourceNode = null;
        _avAudioEngine = null;
        _audioFormat = null;

        _logger.LogInformation("iOS audio resources disposed");
      }
      catch (Exception ex)
      {
        _logger.LogError(ex, "Error disposing iOS audio");
      }
    }


    /// <summary>
    /// Render block for AVAudioSourceNode.
    /// Invoked by Core Audio on real-time thread for audio generation.
    /// This is the HOT PATH - must be allocation-free and deterministic.
    /// </summary>
    /// <returns>0 for success (noErr), non-zero for error.</returns>
    private unsafe int RenderBlock(
      ref bool isSilence,
      ref AudioTimeStamp timestamp,
      uint frameCount,
      AudioBuffers audioBuffers
    )
    {
      // Fast path: return silence if requested
      if (isSilence || _engine is null)
        return 0; // noErr

      try
      {
        // Get audio buffer from AudioBuffers
        // Access the first buffer
        AudioBuffer buffer = audioBuffers[0];

        if (buffer.Data == IntPtr.Zero)
          return -1; // Error: no buffer

        unsafe
        {
          float* outputPtr = (float*)buffer.Data;

          // Validate frame count
          if (frameCount > _iosRenderBuffer.Length)
          {
            // Log on background thread (not in render callback)
            Task.Run(() => _logger.LogWarning(
              "Frame count {FrameCount} exceeds buffer size {BufferSize}, clamping",
              frameCount,
              _iosRenderBuffer.Length
            ));

            frameCount = (uint)_iosRenderBuffer.Length;
          }

          // Fill buffer from audio engine (HOT PATH)
          Span<float> renderSpan = _iosRenderBuffer.AsSpan(0, (int)frameCount);
          _engine.FillBuffer(renderSpan);

          // Copy to Core Audio buffer
          for (int i = 0; i < frameCount; i++)
            outputPtr[i] = _iosRenderBuffer[i];

          return 0; // noErr
        }
      }
      catch (Exception ex)
      {
        // CRITICAL: Never throw from audio callback
        // Log on background thread
        Task.Run(() => _logger.LogError(ex, "iOS render callback error"));
        return -1; // Error
      }
    }
  }
}
